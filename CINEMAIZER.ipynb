{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "r6g_Q4bOFo6w",
    "outputId": "5084c63c-5227-4f3c-d7ec-be361db887cc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6517 files belonging to 5 classes.\n",
      "Using 5214 files for training.\n",
      "Found 6517 files belonging to 5 classes.\n",
      "Using 1303 files for validation.\n",
      "Model: \"MLP\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "random_flip_11 (RandomFlip)  (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "rescaling_11 (Rescaling)     (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 128)               2097280   \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,114,437\n",
      "Trainable params: 2,114,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "163/163 [==============================] - 4s 23ms/step - loss: 1.5423 - accuracy: 0.3021 - val_loss: 1.5097 - val_accuracy: 0.3101\n",
      "Epoch 2/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.4651 - accuracy: 0.3460 - val_loss: 1.4924 - val_accuracy: 0.3108\n",
      "Epoch 3/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.4337 - accuracy: 0.3667 - val_loss: 1.4836 - val_accuracy: 0.3200\n",
      "Epoch 4/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.4100 - accuracy: 0.3824 - val_loss: 1.4787 - val_accuracy: 0.3285\n",
      "Epoch 5/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.3859 - accuracy: 0.4043 - val_loss: 1.4811 - val_accuracy: 0.3208\n",
      "Epoch 6/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.3764 - accuracy: 0.4003 - val_loss: 1.4734 - val_accuracy: 0.3285\n",
      "Epoch 7/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.3527 - accuracy: 0.4223 - val_loss: 1.4761 - val_accuracy: 0.3239\n",
      "Epoch 8/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.3436 - accuracy: 0.4265 - val_loss: 1.4707 - val_accuracy: 0.3246\n",
      "Epoch 9/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.3218 - accuracy: 0.4388 - val_loss: 1.4719 - val_accuracy: 0.3262\n",
      "Epoch 10/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.3121 - accuracy: 0.4438 - val_loss: 1.4642 - val_accuracy: 0.3308\n",
      "Epoch 11/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.2993 - accuracy: 0.4641 - val_loss: 1.4654 - val_accuracy: 0.3346\n",
      "Epoch 12/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.2699 - accuracy: 0.4764 - val_loss: 1.4708 - val_accuracy: 0.3384\n",
      "Epoch 13/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.2609 - accuracy: 0.4812 - val_loss: 1.4666 - val_accuracy: 0.3438\n",
      "Epoch 14/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.2454 - accuracy: 0.4994 - val_loss: 1.4704 - val_accuracy: 0.3477\n",
      "Epoch 15/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.2343 - accuracy: 0.5044 - val_loss: 1.4707 - val_accuracy: 0.3392\n",
      "Epoch 16/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.2248 - accuracy: 0.5059 - val_loss: 1.4695 - val_accuracy: 0.3438\n",
      "Epoch 17/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.1978 - accuracy: 0.5265 - val_loss: 1.4696 - val_accuracy: 0.3438\n",
      "Epoch 18/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.1999 - accuracy: 0.5203 - val_loss: 1.4700 - val_accuracy: 0.3515\n",
      "Epoch 19/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.1803 - accuracy: 0.5418 - val_loss: 1.4711 - val_accuracy: 0.3446\n",
      "Epoch 20/20\n",
      "163/163 [==============================] - 1s 5ms/step - loss: 1.1705 - accuracy: 0.5474 - val_loss: 1.4714 - val_accuracy: 0.3507\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# image dataset parameters\n",
    "val_size = 0.2  \n",
    "batch_size = 32\n",
    "img_height = 128 \n",
    "img_width = 128 \n",
    "\n",
    "# create training dataset\n",
    "train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  \"Data\",\n",
    "  validation_split = val_size,\n",
    "  subset = \"training\",\n",
    "  seed = 123,\n",
    "  color_mode = 'grayscale',\n",
    "  image_size = (img_height, img_width),\n",
    "  batch_size = batch_size)\n",
    "\n",
    "# create validation dataset\n",
    "val_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  \"Data\",\n",
    "  validation_split = val_size,\n",
    "  subset = \"validation\",\n",
    "  seed = 123,\n",
    "  color_mode = \"grayscale\",\n",
    "  image_size = (img_height, img_width),\n",
    "  batch_size = batch_size)\n",
    "\n",
    "# optimisation\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_data = train_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_data = val_data.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# neural network parameters\n",
    "hidden_units = 128\n",
    "dropout_rate = 0.2  \n",
    "learning_rate = 1e-5  \n",
    "\n",
    "# creating neural network (MLP model)\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.Input(shape=(img_height, img_width, 1)),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "  tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset=-1),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "  tf.keras.layers.Dropout(rate=dropout_rate),\n",
    "  tf.keras.layers.Dense(hidden_units, activation='relu'),\n",
    "  tf.keras.layers.Dense(5)\n",
    "], name=\"MLP\")\n",
    "\n",
    "model.compile(\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "  loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics = [\"accuracy\"]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "history = model.fit(train_data,validation_data=val_data,epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WRZhEXXaT2qC"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "1_UtvO6fFo6y",
    "outputId": "1164d64f-1497-402a-b47c-d2246de2ff74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 files belonging to 1 classes.\n",
      "Using 0 files for validation.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002130CB57048> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 - 0s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.86872876,  0.21257624,  0.32524022,  0.7548242 , -1.3887922 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# attempt to make prediction based on inputted image\n",
    "\n",
    "input_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  \"test\",\n",
    "  validation_split=val_size,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  color_mode='grayscale',\n",
    "  image_size=(128, 128),\n",
    "  batch_size=1)\n",
    "\n",
    "model.predict(\n",
    "    input_data,\n",
    "    batch_size=None,\n",
    "    verbose=2,\n",
    "    steps=None,\n",
    "    callbacks=None,\n",
    "    use_multiprocessing=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "vyUVoYHMFo6y",
    "outputId": "7fb1f83d-d205-4211-8520-c747ff2ffb44",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This image most likely belongs to Drama with a 44.11 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "# attempt 2 to create inputted image\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "class_names = [\"Action\", \"Animation\", \"Comedy\", \"Drama\", \"Horror\"]\n",
    "img = tf.keras.utils.load_img(\n",
    "    \"test/bee/bee-movie-2007-us-poster.jpg\", \n",
    "    target_size=(128, 128)\n",
    ")\n",
    "img_array = tf.keras.utils.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "y4YLMTznFo6y",
    "outputId": "fcf45914-fa21-43ff-eebc-c8c79f448a58",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"MLP\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "random_flip_10 (RandomFlip)  (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "rescaling_10 (Rescaling)     (None, 128, 128, 1)       0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               2097280   \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 5)                 645       \n",
      "=================================================================\n",
      "Total params: 2,114,437\n",
      "Trainable params: 2,114,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# save and load model\n",
    "\n",
    "model.save('cinemaizer.h5')\n",
    "tf.keras.models.load_model('cinemaizer.h5').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
